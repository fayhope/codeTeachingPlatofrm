{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "GCCk8_dHpuNf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of code's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*.\n",
        "\n",
        "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) and [eager execution](https://www.tensorflow.org/guide/eager). The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the code dataset\n",
        "\n",
        "Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "if x > 10:\n",
            "    print(\"x is greater than 10\")\n",
            "else:\n",
            "    print(\"x is 10 or less\")\n",
            "\n",
            "x = 12\n",
            "x is greater than 10\n",
            "\n",
            "x = 8\n",
            "x is 10 or less\n",
            "\n",
            "while x < 5:\n",
            "    print(f\"x is {x}\")\n",
            "    x += 1\n",
            "\n",
            "x = 2\n",
            "x is 2\n",
            "x is 3\n",
            "x is 4\n",
            "\n",
            "if y == 0:\n",
            "    print(\"y is zero\")\n",
            "elif y < 0:\n",
            "    print(\"y is negative\")\n",
            "else:\n",
            "    print(\"y is positive\")\n",
            "\n",
            "y = 0\n",
            "y is zero\n",
            "\n",
            "y = -3\n",
            "y is negative\n",
            "\n",
            "y = 7\n",
            "y is positive\n",
            "\n",
            "if z == 5:\n",
            "    print(\"z is equal to 5\")\n",
            "else:\n",
            "    print(\"z is not 5\")\n",
            "\n",
            "z = 5\n",
            "z is equal to 5\n",
            "\n",
            "z = 3\n",
            "z is not 5\n",
            "\n",
            "for i in range(3):\n",
            "    print(f\"i is {i}\")\n",
            "\n",
            "i = 0\n",
            "i is 0\n",
            "i is 1\n",
            "i is 2\n",
            "\n",
            "x = 15\n",
            "if x % 2 == 0:\n",
            "    print(\"x is even\")\n",
            "else:\n",
            "    print(\"x is odd\")\n",
            "\n",
            "x = 15\n",
            "x is odd\n",
            "\n",
            "x = 14\n",
            "x is even\n",
            "\n",
            "while x > 0:\n",
            "    print(f\"x is {x}\")\n",
            "    x -= 2\n",
            "\n",
            "x = 12\n",
            "x is 12\n",
            "x is 10\n",
            "x is 8\n",
            "x is 6\n",
            "x is 4\n",
            "x is 2\n",
            "\n",
            "if y > 0:\n",
            "    print(\"y is positive\")\n",
            "elif y == 0:\n",
            "    print(\"y is zero\")\n",
            "else:\n",
            "    print(\"y is negative\")\n",
            "\n",
            "y = 4\n",
            "y is positive\n",
            "\n",
            "y = 0\n",
            "y is zero\n",
            "\n",
            "y = -1\n",
            "y is negative\n",
            "\n",
            "for j in range(1, 6):\n",
            "    print(f\"j is {j}\")\n",
            "\n",
            "j = 1\n",
            "j is 1\n",
            "j is 2\n",
            "j is 3\n",
            "j is 4\n",
            "j is 5\n",
            "\n",
            "x = 20\n",
            "if x > 10 and x < 30:\n",
            "    print(\"x is between 10 and 30\")\n",
            "else:\n",
            "    print(\"x is outside the range\")\n",
            "\n",
            "x = 20\n",
            "x is between 10 and 30\n",
            "\n",
            "x = 35\n",
            "x is outside the range\n",
            "\n",
            "Length of text: 1192 characters\n"
          ]
        }
      ],
      "source": [
        "# Specify the local path to the file\n",
        "file_path = './code.txt'\n",
        "\n",
        "# Open and read the file\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Print the content or use it for training\n",
        "print(text)\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation. \n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized text (first 100 characters): [11  3 16 75 10  2 43 32 76 12 10  2  8 38 63  3  4 25  3  2 43 32  8  3\n",
            "  4 48  3  2  8 38 64 29  3 23 49 33  2 26  3 80 20  3  4  7  3  2  7  3\n",
            "  2 24  3  2 19 11  5 17 13  9  2 27 46  5 23 13  9  2 40 12  9  2 35  5\n",
            "  4 21  5  2 28  5  4 78  5  2 41  5  4 67  5  2 36 11 14 17 49 34  2 45\n",
            " 30 50 12 34]\n",
            "Decoded text (first 100 characters): [b'if' b'x' b'>' b'10:' b'print(\"x' b'is' b'greater' b'than' b'10\")'\n",
            " b'else:' b'print(\"x' b'is' b'10' b'or' b'less\")' b'x' b'=' b'12' b'x'\n",
            " b'is' b'greater' b'than' b'10' b'x' b'=' b'8' b'x' b'is' b'10' b'or'\n",
            " b'less' b'while' b'x' b'<' b'5:' b'print(f\"x' b'is' b'{x}\")' b'x' b'+='\n",
            " b'1' b'x' b'=' b'2' b'x' b'is' b'2' b'x' b'is' b'3' b'x' b'is' b'4' b'if'\n",
            " b'y' b'==' b'0:' b'print(\"y' b'is' b'zero\")' b'elif' b'y' b'<' b'0:'\n",
            " b'print(\"y' b'is' b'negative\")' b'else:' b'print(\"y' b'is' b'positive\")'\n",
            " b'y' b'=' b'0' b'y' b'is' b'zero' b'y' b'=' b'-3' b'y' b'is' b'negative'\n",
            " b'y' b'=' b'7' b'y' b'is' b'positive' b'if' b'z' b'==' b'5:' b'print(\"z'\n",
            " b'is' b'equal' b'to' b'5\")' b'else:' b'print(\"z']\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Preprocess the code (lowercase and remove extra spaces)\n",
        "def preprocess_code(code):\n",
        "    # Convert text to lowercase\n",
        "    code = code.lower()\n",
        "    # Optionally, remove extra spaces or special symbols\n",
        "    code = re.sub(r'\\s+', ' ', code)  # Replace multiple spaces with a single space\n",
        "    code = re.sub(r'[^\\x00-\\x7F]+', '', code)  # Remove non-ASCII characters (optional)\n",
        "    return code\n",
        "\n",
        "# Preprocess the text from the file\n",
        "text = preprocess_code(text)\n",
        "\n",
        "# Step 3: Vectorize the text at the character level using TensorFlow\n",
        "# Create a TextVectorization layer for character-level tokenization\n",
        "vectorizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=None,  # Don't standardize (we've already processed)\n",
        "    output_mode='int',  # Return indices of characters\n",
        "    output_sequence_length=100  # Set a sequence length limit\n",
        ")\n",
        "\n",
        "# Adapt the vectorizer to the text (fit on the text)\n",
        "vectorizer.adapt([text])\n",
        "\n",
        "# Step 4: Vectorize the text\n",
        "vectorized_text = vectorizer(text)\n",
        "print(f\"Vectorized text (first 100 characters): {vectorized_text[:100]}\")\n",
        "\n",
        "# Step 5: Ensure that the [UNK] token is at the start of the vocabulary\n",
        "vocabulary = vectorizer.get_vocabulary()\n",
        "\n",
        "# If [UNK] is not the first token, move it to the first position\n",
        "if vocabulary[0] != '[UNK]':\n",
        "    vocabulary = ['[UNK]'] + [token for token in vocabulary if token != '[UNK]']\n",
        "\n",
        "# Step 6: Create StringLookup layer with the corrected vocabulary\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=vocabulary, invert=True, mask_token=None\n",
        ")\n",
        "\n",
        "# Decode the vectorized text back into characters\n",
        "decoded_text = chars_from_ids(vectorized_text)\n",
        "print(f\"Decoded text (first 100 characters): {decoded_text.numpy()[:100]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UopbsKi88tm5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All character indices: [15  0  0 ...  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=vocabulary, mask_token=None)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Display the result\n",
        "print(f\"All character indices: {all_ids.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cjH5v45-yqqH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i\n",
            "[UNK]\n",
            "[UNK]\n",
            "x\n",
            "[UNK]\n",
            ">\n",
            "[UNK]\n",
            "1\n",
            "0\n",
            "[UNK]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-05 14:40:50.467777: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BpdjRO2CzOfZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'i' b'[UNK]' b'[UNK]' b'x' b'[UNK]' b'>' b'[UNK]' b'1' b'0' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'i' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'x'\n",
            " b'[UNK]' b'i' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'1' b'0' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'i' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'x' b'[UNK]' b'i' b'[UNK]' b'[UNK]' b'1' b'0' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'x' b'[UNK]' b'=' b'[UNK]' b'1' b'2' b'[UNK]' b'x'\n",
            " b'[UNK]' b'i' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
            " b'[UNK]' b'1' b'0' b'[UNK]' b'x'], shape=(101,), dtype=string)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-05 14:40:50.557400: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WxbDTJTw5u_P"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "GNbw-iR0ymwj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input  : ix>10ixi10ixi10x=12xi10\n",
            "Target : x>10ixi10ixi10x=12xi10x\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-05 14:40:51.687191: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "# Define the reverse StringLookup layer for decoding the indices back to characters\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocabulary, invert=True, mask_token=None)\n",
        "\n",
        "# Step 1: Example dataset with input and target sequences\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    # Decode the input and target sequences using chars_from_ids\n",
        "\n",
        "    # Convert tensors to numpy arrays of integers\n",
        "    input_text = input_example.numpy()  # input_example is already a tensor of integers\n",
        "    target_text = target_example.numpy()  # target_example is already a tensor of integers\n",
        "\n",
        "    # Decode the numpy array of indices to characters\n",
        "    input_text_decoded = ''.join([vocabulary[i] if i != 0 else '' for i in input_text])  # Ignore padding (0)\n",
        "    target_text_decoded = ''.join([vocabulary[i] if i != 0 else '' for i in target_text])  # Ignore padding (0)\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Input  :\", input_text_decoded)\n",
        "    print(\"Target :\", target_text_decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "tokenizer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation')\n",
        "tokenizer.adapt(text)\n",
        "\n",
        "vocab_size = len(tokenizer.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding output shape: (1, 10, 256)\n",
            "Initialized states shape: (1, 1024)\n",
            "GRU output shape: (1, 10, 1024)\n",
            "GRU states shape: (1, 1024)\n",
            "Dense output shape: (1, 10, 55)\n",
            "Embedding output shape: (1, 10, 256)\n",
            "Initialized states shape: (1, 1024)\n",
            "GRU output shape: (1, 10, 1024)\n",
            "GRU states shape: (1, 1024)\n",
            "Dense output shape: (1, 10, 55)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"my_model_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                 │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">56,375</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │        \u001b[38;5;34m14,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ ((\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m1\u001b[0m,    │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m1024\u001b[0m))                 │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m55\u001b[0m)            │        \u001b[38;5;34m56,375\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,008,759</span> (15.29 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,008,759\u001b[0m (15.29 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,008,759</span> (15.29 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,008,759\u001b[0m (15.29 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        y = inputs\n",
        "        x = self.embedding(y, training=training)\n",
        "        print(f\"Embedding output shape: {x.shape}\")\n",
        "\n",
        "        if states is None:\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            states = [tf.zeros((batch_size, self.gru.units))]\n",
        "            print(f\"Initialized states shape: {states[0].shape}\")\n",
        "\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        print(f\"GRU output shape: {x.shape}\")\n",
        "        print(f\"GRU states shape: {states.shape}\")\n",
        "\n",
        "        x = self.dense(x, training=training)\n",
        "        print(f\"Dense output shape: {x.shape}\")\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        return x\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")\n",
        "\n",
        "# Create a dummy input with batch_size=1 and sequence_length=10\n",
        "dummy_input = tf.random.uniform(shape=(1, 10), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
        "\n",
        "# Perform a forward pass to initialize the model\n",
        "model(dummy_input)\n",
        "\n",
        "# Now the model should be built and you can check the summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](images/text_generation_training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "C-_70kKAPrPU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_TakeDataset element_spec=(TensorSpec(shape=(None, None, None, 64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, None, None, 64, 100), dtype=tf.int64, name=None))>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-05 15:15:10.899011: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Shuffle and batch the dataset (without repeat)\n",
        "dataset = dataset.shuffle(10).batch(10)\n",
        "\n",
        "# Define number of steps to iterate over the dataset (e.g., 100 steps)\n",
        "num_steps = 100\n",
        "step_counter = 0\n",
        "\n",
        "# Manually control the number of steps\n",
        "for input_example_batch, target_example_batch in dataset.take(num_steps):\n",
        "    print(\"hello\")\n",
        "    # Make predictions with the model\n",
        "    example_batch_predictions = model(input_example_batch, training=False)\n",
        "    \n",
        "    # Print the shape of the predictions\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "    # Calculate the loss for this batch\n",
        "    example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "    print(\"Mean loss:        \", example_batch_mean_loss.numpy())\n",
        "\n",
        "    # Sample indices from the predictions (probabilities)\n",
        "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "    sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "    # Convert input batch and sampled predictions to text\n",
        "    print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "    print()\n",
        "    print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices))\n",
        "\n",
        "    # Increment step counter\n",
        "    step_counter += 1\n",
        "\n",
        "    # If you reach the desired number of steps, break the loop\n",
        "    if step_counter >= num_steps:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4HrXTACTdzY-"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'target_example_batch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_batch_mean_loss \u001b[38;5;241m=\u001b[39m loss(\u001b[43mtarget_example_batch\u001b[49m, example_batch_predictions)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, example_batch_predictions\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m # (batch_size, sequence_length, vocab_size)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean loss:        \u001b[39m\u001b[38;5;124m\"\u001b[39m, example_batch_mean_loss)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target_example_batch' is not defined"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "MAJfS5YoFiHf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.93071"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoints will be saved at: ./training_checkpoints/ckpt_{epoch}.weights.h5\n"
          ]
        }
      ],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files (with proper extension)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "print(f\"Checkpoints will be saved at: {checkpoint_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UK-hmKjYVoll"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'checkpoint_callback' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(dataset, epochs\u001b[38;5;241m=\u001b[39mEPOCHS, callbacks\u001b[38;5;241m=\u001b[39m[\u001b[43mcheckpoint_callback\u001b[49m], steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_callback' is not defined"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], steps_per_epoch=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](images/text_generation_sampling.png)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a code-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "ST7PSyk9t1mT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding output shape: (1, None, 256)\n",
            "Initialized states shape: (1, 1024)\n",
            "GRU output shape: (1, None, 1024)\n",
            "GRU states shape: (1, 1024)\n",
            "Dense output shape: (1, None, 66)\n",
            "Embedding output shape: (1, None, 256)\n",
            "GRU output shape: (1, None, 1024)\n",
            "GRU states shape: (1, 1024)\n",
            "Dense output shape: (1, None, 66)\n",
            "ROMEO: Came.\n",
            "\n",
            "CLARENCE:\n",
            "Wherewo have a mother dies to slave, do\n",
            "not how the law oth the faces as\n",
            "up, I have lend between air.\n",
            "O pity, I have power to town it\n",
            "Tust Slabext, tarry in this trie;;\n",
            "And I, who stopless. Is it is my lord\n",
            "Inceest thou intencements, ne'er ane of mine?\n",
            "\n",
            "DUKE OF AUMELLE:\n",
            "I cannot made, am I see forthy desires\n",
            "Promised war and life, brought to old confer's me.\n",
            "\n",
            "Betteres: they not longo me with what he wings?\n",
            "\n",
            "AUTONCLARD:\n",
            "I know deniment leakness, for his lime were I\n",
            "And follow nuts, being good malicate.\n",
            "\n",
            "FRIAR:\n",
            "Hast I a leave a comblacomrous, and I moot\n",
            "And help me with give against said tha.\n",
            "Farely said, full of some gutyen'd:\n",
            "My life, for soon brack sheWhing thy mind;\n",
            "Wife unto the Realios Against thee, is calliant;\n",
            "Would eave perse scocest woeship, withal both\n",
            "consurs! Her canst by commistion'd ye.\n",
            "Nay, death, I prote; and best bess with\n",
            "Filty untignerlainess to Sickn.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Are shall less redeemmemnets I'll be confessed on.\n",
            "\n",
            "MARCIUS:\n",
            "Pray, good better, st \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.6551189422607422\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "ZkLu7Y8UCMT7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding output shape: (5, None, 256)\n",
            "Initialized states shape: (5, 1024)\n",
            "GRU output shape: (5, None, 1024)\n",
            "GRU states shape: (5, 1024)\n",
            "Dense output shape: (5, None, 66)\n",
            "Embedding output shape: (5, None, 256)\n",
            "GRU output shape: (5, None, 1024)\n",
            "GRU states shape: (5, 1024)\n",
            "Dense output shape: (5, None, 66)\n",
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhat is't not, by been?\\n\\nCATESBY:\\nYou were redance, my lord: a mocking his pardly or you.\\n\\nDUKE OF AUMERLE:\\nEven fibther, my lord: with him, and say'st\\nWarwick, yent; For fled, now, pleased,\\nHis shade here plospess'd Throughted, though any need.\\nHath hence a man belefit it.\\nLook, it is a cheer. Why, you spoke im\\nFirst unkingness cares not sleeping; nowly!\\n\\nMOFF EDWARD:\\nThat is this? hence? Say no; I come to med, bit thou cive?\\n\\nPRocHOR OF GLURET:\\nWere on't you are comes come again:\\nWith my partine Canison, I had recevert!\\n\\nFurst:\\nHe's nor not our aspair.\\n\\nGONZALO:\\nTake my lord, I soom more to mird Citizen:\\nI'll hand resemble with be thy chrick\\nYielding-and one sucpled spares, nerved that,\\nWhere's Menerius with you. Commen, fair! where he of Norfolk,\\nAnd morrow, for perch me this: my libery,\\ncomes on till be Thange to rejover it.\\n\\nHirst Sit:\\nAn, merry you, lets Think up by me;\\nI am A kingly father, wonger;\\nThe foes, did yet be some part\\nBut laid thee pleased your grace can you perce,--\"\n",
            " b\"ROMEO:\\nSir Jombin: go you are.\\n\\nRATCLIFF:\\nCome, I had not can pue it,\\nWhere hard talk off! I wast delighful me.\\n\\nLADY ANN\\nThen I must churches it.\\n\\nISABELLA:\\nIs not the butt, great like for their fullsome.\\n\\nPAMILLO:\\nI for thy matter, lief false hands, may so read\\nUSped a lighted him instantion and hither\\nAnd not let her do attend their cart,.\\n\\nBRUTUS:\\nNignius?\\n\\nPood:\\nFallowing!\\n\\nDUKE OF YORK:\\nPracation by me likes.\\n\\nMARIANA:\\nAury with the hourd his marrizing men\\nInsurp'd his put that speakn an unupposed you?\\nFor this muscive, and some father,\\nLook they scrows, his mangshipher oppray ones\\nSaid to you will be Hereford'st where how with me fight.\\nSay, think you heard with him.\\n\\nBAPTISTA:\\nNothing there fell a thing.\\n\\nPRINCE EDWARD:\\nMy palery, parmon my fair, the sun well it be\\nBy knem thou evertion,--withouted what it\\nLend to pleck'd old as this: thou regedst;\\nTo three desperite and Warwick,\\nAnd therefore I never regelved e'll\\nsir. Pot off you rowness have go?\\n\\nMRINCE:\\nWhat she said, fair break \"\n",
            " b\"ROMEO:\\nHis noble Hastings come to your crow, and,\\nThe letter of your king. Cergent thou, wood, I'll give you are pray thee\\nwhere was met with ull poss to me.\\n\\nANTONIO:\\nFor, poor brother, be believe thy head.\\n\\nCLARENCE:\\nOne that I had said you were for-exert.\\n\\nROMEO:\\nSay you can think me and a little.\\n\\nLEONTES:\\nDo; good beldy: his\\nmost scargeless rememedain.\\n\\nHASTINGS:\\nWhat,'th your esteny?\\n\\nAll:\\nWelcome! With you, Lancis,\\nLook't prouded my laby-ready, there,\\nFor one the worth.\\n\\nLord:\\nMide are to ear, and ade way, give\\nRichard most; or logeth noth such\\nappanition till with perjuse mon?\\nThou creature, can for the friendls fearful,\\nAny comes by ourself athern ange,\\nBith he know me may my love?\\nBetwixt thou art preyen o' thy black--hermans amich\\nThe minishment, I am thou time\\nmy furlifives call this man perdon by Like as these\\nThan pit at like by the young Hangs\\nOf the oteer-villains and unhortance this ore\\nThere line converger and unruching, putituful.\\n\\nHERMIONDE:\\nMy lords borns: I sel gain:\\nI \"\n",
            " b\"ROMEO:\\nMistard, profess rese; and\\nthe purges, came be eny Warwick prize\\nAnd in the way behind might:\\nEven then will bail thy duly, that never pounder\\nAnd hund: the mother o'ercaningle?\\n\\nQUEEE:\\nNow, why do go:\\nYou may content you indeed,\\nNor worb! where' think I walk off thy breath;\\nPot majesty burshy so greated men,\\nWhen I hear our tables, old gabler,\\nWhich thou know not made your voluce, remains.\\n\\nLADY GREY:\\nWhy, 'Sigh is youth. Gentlemen, by batter son, stay;\\nThere we may be crown. To make you not head to holy in a plight;\\nBetwain my some tale me dreams. When they! may shall you I\\nTill dely fierch! Whose houses\\nDo how with pilling is thy like give: the\\nCations? till I say?\\n\\nQUEEN MARGARET:\\nNo, and my lord,--\\nAnd crewh thee, do intend wor such far.\\n\\nDUKE OV EAT:\\nWell, I meet it: not attendy now,\\nI'll keep you, William between you to be\\nmy lord, to dark'd upon the sacrists, is you are.\\nGood, horse unwill sorrow are me to my suit\\nAnd so my couss, uppise thee!\\n\\nLADY ANNE:\\nI have so dicher at t\"\n",
            " b\"ROMEO:\\nAnd where I am, thangeThy breather of Gear.\\nWalks, murditness, and I remember yea, and content\\nTowards fur againsa there; then made breathe despect,\\nhis perish her cool quince and turn's fresh the mory pleases;\\nBut gentle are out\\nSit morines by mir, auting in myself\\nAnd stity to cleat youn Rameloo; hath we'll please you.\\n\\nCARISABES:\\nMethonger you.\\n\\nABTOGS:\\nAt heaven of us ancient inmague,\\nAnd hend thes hands to a malreapary. Istend me upon thee,\\nAnd now aUdled it to bitter thee desert; now deeds\\nThee, may then to merce to sleep.\\n\\nSMY: But set on me?\\n\\nNurse:\\nWhy, Romeo, comple!\\n\\nSICINIUS:\\n'Tis think\\nIn merry disposted's ghost worse; dost and give\\nI would be the Rifhand shaft those, the brearth her sing.\\n\\nANTEGOO:\\nI hope to stay.\\n\\nANGELO:\\nBranquit on\\nyo mad thou darest traturned by no move insur town.\\n\\nLARCIUS:\\nAll six's him, I am myself,\\nand I would take to-morrow nead my requinalen walls;\\nDost thou med may will inters shall dishoids.\\n\\nCOMINIUS:\\nAchorp us swift?\\n\\nHONTENSII:\\nMawages,\\nIf\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.1876368522644043\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text_generation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
